
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>SQIL: Saliency-aware Quantized Imitation Learning</title>
  <link rel="stylesheet" href="css/style.css">
</head>
<body>
  <header class="header">
    <h1>SQIL:</h1>
    <h2>Saliency-aware Quantized Imitation Learning</h2>
    <p><strong>Under Review at ICCV 2025</strong></p>
    <p class="links">
      <a href="https://arxiv.org/abs/2505.15304">[Paper]</a>
      <a href="https://github.com/your-org/sqil">[Code]</a>
      <a href="assets/demo.mp4">[Video]</a>
    </p>
  </header>

  <section class="teaser">
    <img src="assets/fig_1.PNG" alt="Teaser Image">
  </section>

  <section class="content">
    <h2>Abstract</h2>
    <p>Deep neural network (DNN)-based policy models, such as vision-language-action (VLA) models, excel at automating complex decision-making from multi-modal inputs. However, scaling these models greatly increases computational overhead, complicating deployment in resource-constrained settings like robot manipulation and autonomous driving. To address this, we propose Saliency-Aware Quantized Imitation Learning (SQIL), which combines quantization-aware training with a selective loss-weighting strategy for mission-critical states. By identifying these states via saliency scores and emphasizing them in the training loss, SQIL preserves decision fidelity under low-bit precision. We validate SQIL's generalization capability across extensive simulation benchmarks with environment variations, real-world tasks, and cross-domain tasks (self-driving, physics simulation), consistently recovering full-precision performance. Notably, a 4-bit weight-quantized VLA model for robotic manipulation achieves up to 2.5x speedup and 2.5x energy savings on an edge GPU with minimal accuracy loss. These results underline SQILâ€™s potential for efficiently deploying large IL-based policy models on resource-limited devices.</p>

    <h2>Highlights</h2>
    <ul>
      <li>4-bit policy with minimal accuracy drop</li>
      <li>State importance via perturbation saliency</li>
      <li>Robust generalization across simulation and real-world tasks</li>
    </ul>
  <section class="grid-section">
    <h2>LIBERO-Long Task: put the black bowl in the drawer</h2>
    <div class="video-grid">
      <div class="item">
        <h3>Baseline FP</h3>
        <video controls width="100%">
          <source src="assets/fp.mp4" type="video/mp4">
        </video>
        <p class="caption success">Success</p>
      </div>
      <div class="item">
        <h3>PTQ W4</h3>
        <video controls width="100%">
          <source src="assets/ptq.mp4" type="video/mp4">
        </video>
        <p class="caption fail">Failed to put into the drawer</p>
      </div>
      <div class="item">
        <h3>QAT W4</h3>
        <video controls width="100%">
          <source src="assets/qat.mp4" type="video/mp4">
        </video>
        <p class="caption fail">Failed to put the bowl upright into the drawer</p>
      </div>
      <div class="item">
        <h3>SQIL W4 (Ours)</h3>
        <video controls width="100%">
          <source src="assets/sqil.mp4" type="video/mp4">
        </video>
        <p class="caption success">Success</p>
      </div>
    </div>
  </section>

    <h2>Demo Video</h2>
    <video controls width="100%">
      <source src="assets/demo.mp4" type="video/mp4">
      Your browser does not support the video tag.
    </video>

    <h2>BibTeX</h2>
    <pre>
@inproceedings{park2025sqil,
  title={Saliency-aware Quantized Imitation Learning},
  author={Park, Seongmin and ...},
  booktitle={ICCV},
  year={2025}
}
    </pre>
  </section>

  <footer>
    <p>&copy; 2025 Seongmin Park, VLA Lab</p>
  </footer>
</body>
</html>
