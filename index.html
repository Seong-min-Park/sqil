<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>SQIL: Saliency-aware Quantized Imitation Learning</title>
  <link rel="stylesheet" href="css/style.css">
  <style>
    .video-grid {
      display: grid;
      grid-template-columns: repeat(4, 1fr);
      gap: 1.5em;
      margin-top: 1em;
    }
    .item {
      text-align: center;
    }
    .caption {
      margin-top: 0.5em;
      padding: 0.4em;
      border-radius: 8px;
      font-weight: bold;
    }
    .caption.success {
      background-color: #fef2e0;
      color: green;
    }
    .caption.fail {
      background-color: #fdeaea;
      color: red;
    }
    .rollout-section {
      margin-top: 3em;
    }
    .rollout-section h2 {
      border-bottom: 2px solid #ddd;
      padding-bottom: 0.3em;
    }
    .task-navigation {
      text-align: center;
      margin: 1em 0;
    }
    .task-navigation button {
      padding: 5px 12px;
      margin: 0 10px;
      cursor: pointer;
    }
    .image-grid {
      display: grid;
      grid-template-columns: repeat(2, 1fr);
      gap: 2em;
      margin-top: 1em;
    }
    .autonomous-driving {
      display: flex;
      flex-direction: column;
      gap: 1em;
      align-items: center;
      margin-top: 2em;
    }
    .autonomous-driving img {
      width: 100%;
      max-width: 900px;
      border: 1px solid #ccc;
    }
  </style>
</head>
<body>
  <header class="header">
    <h1>SQIL:</h1>
    <h2>Saliency-aware Quantized Imitation Learning</h2>
    <p><strong>Under Review at ICCV 2025</strong></p>
    <p class="links">
      <a href="https://arxiv.org/abs/2505.15304">[Paper]</a>
      <a href="https://github.com/your-org/sqil">[Code]</a>
      <a href="assets/demo.mp4">[Video]</a>
    </p>
  </header>

  <section class="teaser">
    <img src="assets/fig_1.PNG" alt="Teaser Image">
  </section>

  <section class="content">
    <h2>Abstract</h2>
    <p>Deep neural network (DNN)-based policy models, such as vision-language-action (VLA) models, excel at automating complex decision-making from multi-modal inputs. However, scaling these models greatly increases computational overhead, complicating deployment in resource-constrained settings like robot manipulation and autonomous driving. To address this, we propose Saliency-Aware Quantized Imitation Learning (SQIL), which combines quantization-aware training with a selective loss-weighting strategy for mission-critical states. By identifying these states via saliency scores and emphasizing them in the training loss, SQIL preserves decision fidelity under low-bit precision. We validate SQIL's generalization capability across extensive simulation benchmarks with environment variations, real-world tasks, and cross-domain tasks (self-driving, physics simulation), consistently recovering full-precision performance. Notably, a 4-bit weight-quantized VLA model for robotic manipulation achieves up to 2.5x speedup and 2.5x energy savings on an edge GPU with minimal accuracy loss. These results underline SQILâ€™s potential for efficiently deploying large IL-based policy models on resource-limited devices.</p>

    <h2>Highlights</h2>
    <ul>
      <li>4-bit policy with minimal accuracy drop</li>
      <li>State importance via perturbation saliency</li>
      <li>Robust generalization across simulation and real-world tasks</li>
    </ul>

    <div class="rollout-section">
      <h2>Rollout Video</h2>

      <h3>Real-World Robot Manipulation</h3>
      <div class="task-navigation">
        <button onclick="showTask('real1')">Task 1</button>
        <button onclick="showTask('real2')">Task 2</button>
      </div>
      <div id="real1" class="video-grid">
        <div class="item"><h4>Baseline FP</h4><video src="assets/real1_fp.mp4" autoplay muted loop></video><p class="caption success">Success</p></div>
        <div class="item"><h4>PTQ W4</h4><video src="assets/real1_ptq.mp4" autoplay muted loop></video><p class="caption fail">Failed</p></div>
        <div class="item"><h4>SQIL W4</h4><video src="assets/real1_sqil.mp4" autoplay muted loop></video><p class="caption success">Success</p></div>
      </div>
      <div id="real2" class="video-grid" style="display:none">
        <div class="item"><h4>Baseline FP</h4><video src="assets/real2_fp.mp4" autoplay muted loop></video><p class="caption success">Success</p></div>
        <div class="item"><h4>PTQ W4</h4><video src="assets/real2_ptq.mp4" autoplay muted loop></video><p class="caption fail">Failed</p></div>
        <div class="item"><h4>SQIL W4</h4><video src="assets/real2_sqil.mp4" autoplay muted loop></video><p class="caption success">Success</p></div>
      </div>

      <h3>Simulation-based Robot Manipulation (LIBERO)</h3>
      <div class="image-grid">
        <img src="assets/libero_spatial.png" alt="Spatial">
        <img src="assets/libero_object.png" alt="Object">
        <img src="assets/libero_goal.png" alt="Goal">
        <img src="assets/libero_long.png" alt="Long">
      </div>

      <h3>Autonomous Driving</h3>
      <div class="autonomous-driving">
        <img src="assets/driving_baseline.png" alt="Driving Baseline">
        <img src="assets/driving_qat.png" alt="Driving QAT">
        <img src="assets/driving_sqil.png" alt="Driving SQIL">
      </div>
    </div>

    <h2>Demo Video</h2>
    <video controls width="100%">
      <source src="assets/demo.mp4" type="video/mp4">
      Your browser does not support the video tag.
    </video>

    <h2>BibTeX</h2>
    <pre>
@inproceedings{park2025sqil,
  title={Saliency-aware Quantized Imitation Learning},
  author={Park, Seongmin and ...},
  booktitle={ICCV},
  year={2025}
}
    </pre>
  </section>

  <footer>
    <p>&copy; 2025 Seongmin Park, VLA Lab</p>
  </footer>

  <script>
    function showTask(id) {
      document.getElementById('real1').style.display = 'none';
      document.getElementById('real2').style.display = 'none';
      document.getElementById(id).style.display = 'grid';
    }
  </script>
</body>
</html>
